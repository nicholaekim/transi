# PDF Metadata Extraction Pipeline

A comprehensive PDF metadata extraction system with interactive batch processing and web interface.

## ğŸ“ Project Structure

```
ocr3.0/
â”œâ”€â”€ ğŸ“ src/                          # Source code
â”‚   â”œâ”€â”€ ğŸ“ core/                     # Core pipeline logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ pipeline.py              # Main PDF processing pipeline
â”‚   â”œâ”€â”€ ğŸ“ utils/                    # Utility scripts
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_checker.py          # Interactive batch processor
â”‚   â””â”€â”€ ğŸ“ web/                      # Web application
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ app.py                   # Flask web app
â”‚       â””â”€â”€ ğŸ“ templates/            # HTML templates
â”‚           â””â”€â”€ index.html
â”œâ”€â”€ ğŸ“ data/                         # Data storage
â”‚   â”œâ”€â”€ ğŸ“ raw/                      # Input PDFs for batch processing
â”‚   â”œâ”€â”€ ğŸ“ processed/                # Processed files
â”‚   â”œâ”€â”€ ğŸ“ results/                  # Batch processing results
â”‚   â””â”€â”€ feedback_memory.json        # User feedback and fine-tuning data
â”œâ”€â”€ ğŸ“ config/                       # Configuration files

## ğŸ“Š Extracted Metadata

- **Title**: Document title or main subject
- **Date**: Publication or creation date
- **Description**: Intelligent summary of content
- **Volume/Issue**: Publication volume and issue numbers
- **Confidence Scores**: Reliability metrics for each extraction
- **Processing Metadata**: Performance analytics and model details

## âš¡ Installation

1. **Install Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Install and start Ollama:**
   ```bash
   # Install Ollama (macOS)
   brew install ollama
   
   # Start Ollama service
   ollama serve
   ```

3. **Install required models:**
   ```bash
   ollama pull granite3.2-vision
   ollama pull llama3.1:8b
   ollama pull llama3.1:70b
   ```

4. **Configure Transkribus API (Optional - for PDF processing):**
   ```bash
   # Copy environment template
   cp .env.example .env
   
   # Edit .env with your Transkribus credentials
   # Get credentials from: https://transkribus.eu/
   ```

## ğŸš€ Quick Start

### Process PDF (with Transkribus API)
```bash
# Process PDF through Transkribus OCR
python3 simple_pipeline.py document.pdf

# This will:
# 1. Upload PDF to Transkribus
# 2. Run OCR processing
# 3. Save extracted text for manual correction
# 4. Prompt you to review and correct the text
# 5. Re-run with corrected text file
```

### Process Text File (already corrected)
```bash
# Process pre-corrected text file
python3 simple_pipeline.py document.txt
```

## ğŸ¯ Usage Modes

### **Parallel Mode (Default - Fastest)**
```bash
python3 simple_pipeline.py document.txt
python3 simple_pipeline.py document.txt --priority speed
```

### **Consensus Mode (Highest Accuracy)**
```bash
python3 simple_pipeline.py document.txt --mode consensus --priority accuracy
```

### **Custom Processing**
```bash
# Skip automatic feedback
python3 simple_pipeline.py document.txt --no-feedback

# Balance speed and accuracy
python3 simple_pipeline.py document.txt --priority balanced
```

## ğŸ”„ Complete Workflow

1. **ğŸ“„ Transkribus Processing**: Upload document â†’ OCR â†’ Manual correction â†’ Export as `.txt`
2. **ğŸ§  Document Analysis**: Automatic type detection and quality assessment
3. **âš¡ Parallel Extraction**: Simultaneous processing of all metadata fields
4. **ğŸ¯ Quality Validation**: Confidence scoring and optional two-pass extraction
5. **ğŸ’¾ Results Storage**: Structured JSON with detailed metadata
6. **ğŸ“Š Automatic Feedback**: Accuracy analysis and improvement suggestions

## ğŸ“ Clean Project Structure

```
ocr/
â”œâ”€â”€ simple_pipeline.py                    # ğŸš€ Main advanced pipeline
â”œâ”€â”€ fine_tune_and_train.py                # ğŸ“Š Feedback & training system
â”œâ”€â”€ src/core/
â”‚   â”œâ”€â”€ enhanced_extractor.py             # ğŸ¤– Advanced LLM extraction
â”‚   â”œâ”€â”€ smart_document_analyzer.py        # ğŸ“‹ Document analysis & segmentation
â”‚   â””â”€â”€ advanced_model_manager.py         # ğŸ§  Intelligent model selection
â”œâ”€â”€ end_pipeline/                         # ğŸ“ Final results (JSON)
â”œâ”€â”€ training_data/                        # ğŸ“š Feedback & improvement data
â””â”€â”€ requirements.txt                      # ğŸ“¦ Minimal dependencies
```

## ğŸ›ï¸ Advanced Options

| Option | Description | Values |
|--------|-------------|--------|
| `--mode` | Extraction strategy | `parallel` (fast), `consensus` (accurate) |
| `--priority` | Processing priority | `speed`, `balanced`, `accuracy` |
| `--no-feedback` | Skip feedback analysis | flag |

## ğŸ“ˆ Performance Features

- **4x Faster**: Parallel processing vs sequential
- **Smart Caching**: Document type and performance caching
- **Dynamic Routing**: Optimal model selection per task
- **Quality Adaptation**: Processing adjusts to text quality
- **Continuous Improvement**: Learning from user corrections

## ğŸ”§ Requirements

- Python 3.8+
- Ollama with models: `granite3.2-vision`, `llama3.1:8b`, `llama3.1:70b`
- 8GB+ RAM recommended for optimal performance

## ğŸ“Š Output Format

Results include enhanced metadata:
```json
{
  "title": "Document Title",
  "date": "2024-01-15",
  "description": "Intelligent summary...",
  "volume_issue": "Volume 3, Issue 2",
  "extraction_metadata": {
    "total_processing_time": 2.34,
    "extraction_method": "parallel",
    "field_details": {
      "title": {
        "confidence": 0.92,
        "model_used": "granite3.2-vision",
        "processing_time": 0.58
      }
    }
  },
  "document_analysis": {
    "document_type": "newsletter",
    "quality": {"overall_quality": 0.87}
  }
}
```
