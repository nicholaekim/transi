# OCR Pipeline v3.0

ğŸš€ **Advanced metadata extraction pipeline with Transkribus integration**

Clean, organized pipeline for extracting structured metadata from documents using AI models and professional OCR.

## ğŸ“ Project Structure

```
ocr/
â”œâ”€â”€ run_pipeline.py          # Main entry point
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ .env                    # Environment variables (create from config/.env.example)
â”‚
â”œâ”€â”€ pipeline/               # Core pipeline code
â”‚   â”œâ”€â”€ simple_pipeline.py  # Main pipeline logic
â”‚   â”œâ”€â”€ core/              # Core modules
â”‚   â”‚   â”œâ”€â”€ advanced_model_manager.py
â”‚   â”‚   â”œâ”€â”€ enhanced_extractor.py
â”‚   â”‚   â”œâ”€â”€ smart_document_analyzer.py
â”‚   â”‚   â””â”€â”€ transkribus_client.py
â”‚   â””â”€â”€ utils/             # Utilities
â”‚       â””â”€â”€ fine_tune_and_train.py
â”‚
â”œâ”€â”€ config/                # Configuration files
â”‚   â””â”€â”€ .env.example       # Environment template
â”‚
â”œâ”€â”€ scripts/               # Setup and testing scripts
â”‚   â”œâ”€â”€ setup.sh          # Automated setup
â”‚   â””â”€â”€ test_workflow.py   # Testing utilities
â”‚
â”œâ”€â”€ data/                  # Data directories
â”‚   â”œâ”€â”€ input/            # Input documents
â”‚   â”œâ”€â”€ output/           # Final results
â”‚   â””â”€â”€ temp/             # Temporary files
â”‚
â””â”€â”€ docs/                 # Documentation
    â””â”€â”€ README.md         # This file
```

## âš¡ Installation

1. **Install Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Install and start Ollama:**
   ```bash
   # Install Ollama (macOS)
   brew install ollama
   
   # Start Ollama service
   ollama serve
   ```

3. **Install required models:**
   ```bash
   ollama pull granite3.2-vision
   ollama pull llama3.1:8b
   ollama pull llama3.1:70b
   ```

4. **Configure Transkribus API (Optional - for PDF processing):**
   ```bash
   # Copy environment template
   cp config/.env.example .env
   
   # Edit .env with your Transkribus credentials
   # Get credentials from: https://transkribus.eu/
   ```

## ğŸš€ Quick Start

### Process PDF (with Transkribus API)
```bash
# Process PDF through Transkribus OCR
python3 run_pipeline.py document.pdf

# This will:
# 1. Upload PDF to Transkribus
# 2. Run OCR processing
# 3. Save extracted text for manual correction
# 4. Prompt you to review and correct the text
# 5. Re-run with corrected text file
```

### Process Text File (already corrected)
```bash
# Process pre-corrected text file
python3 run_pipeline.py document.txt
```

## ğŸ¯ Usage Modes

### Parallel Mode (Default - Fastest)
```bash
python3 run_pipeline.py document.txt --mode parallel
```

### Consensus Mode (Highest Accuracy)
```bash
python3 run_pipeline.py document.txt --mode consensus
```

### Priority Settings
```bash
# Speed optimized
python3 run_pipeline.py document.txt --priority speed

# Balanced (default)
python3 run_pipeline.py document.txt --priority balanced

# Accuracy optimized
python3 run_pipeline.py document.txt --priority accuracy
```

## ğŸ“Š Features

- **ğŸ¤– Advanced AI Models**: Multiple specialized LLMs for different tasks
- **ğŸ“„ PDF Processing**: Transkribus integration for professional OCR
- **ğŸ”„ Two-Pass Extraction**: Consensus validation for accuracy
- **ğŸ“ˆ Smart Document Analysis**: Automatic quality assessment and optimization
- **ğŸ¯ Dynamic Model Selection**: Task-specific model routing
- **ğŸ“ Manual Correction**: Human-in-the-loop workflow
- **ğŸ”§ Automatic Feedback**: Continuous improvement system
- **âš¡ Parallel Processing**: Fast multi-field extraction
- **ğŸ“‹ Confidence Scores**: Reliability metrics for each extraction
- **ğŸ›ï¸ Processing Metadata**: Performance analytics and model details

## ğŸ› ï¸ Automated Setup

```bash
# Run automated setup script
chmod +x scripts/setup.sh
./scripts/setup.sh
```

## ğŸ§ª Testing

```bash
# Run test workflow
python3 scripts/test_workflow.py

# Test with sample document
python3 run_pipeline.py data/temp/sample_document.txt
```

## ğŸ“¤ Output

Results are saved to `data/output/end_results.json` with:
- **Extracted Metadata**: Title, date, description, volume/issue
- **Confidence Scores**: Reliability metrics
- **Processing Details**: Model performance and timing
- **Document Analysis**: Quality metrics and structure info

## ğŸ”§ Advanced Options

```bash
# Skip automatic feedback analysis
python3 run_pipeline.py document.txt --no-feedback

# Full help
python3 run_pipeline.py --help
```

## ğŸ“š Workflow

1. **Input**: PDF or text file
2. **OCR** (if PDF): Transkribus processing
3. **Manual Correction**: Review extracted text
4. **AI Processing**: Multi-model metadata extraction
5. **Validation**: Consensus checking and confidence scoring
6. **Output**: Structured JSON with metadata
7. **Feedback**: Automatic accuracy analysis and improvement

---

**Built with advanced AI models and professional OCR for maximum accuracy and reliability.**
